{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import, analyse and prepare data\n",
    "\n",
    "## Store Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/stores.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I want to make predictions per store, the store data itself is not relevant for my model\n",
    "\n",
    "## Feature Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has to be transformed into the correct datatypes.\n",
    "\n",
    "As MarkDown data is not available for more than half of the records, I decided to neglect this information for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, IntegerType, DateType, BooleanType\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "\n",
    "features = features\\\n",
    "    .withColumn(\"Store\", features.Store.cast(IntegerType()))\\\n",
    "    .withColumn(\"Date\", to_date(features.Date, \"dd/MM/yyyy\"))\\\n",
    "    .withColumn(\"Temperature\", features.Temperature.cast(DoubleType()))\\\n",
    "    .withColumn(\"Fuel_Price\", features.Fuel_Price.cast(DoubleType()))\\\n",
    "    .withColumn(\"CPI\", features.CPI.cast(DoubleType()))\\\n",
    "    .withColumn(\"Unemployment\", features.Unemployment.cast(DoubleType()))\\\n",
    "    .withColumn(\"IsHoliday\", features.IsHoliday.cast(BooleanType()))\n",
    "\n",
    "features = features.select(\"Store\",\\\n",
    "                           \"Date\",\\\n",
    "                           \"Temperature\",\\\n",
    "                           \"Fuel_Price\",\\\n",
    "                           \"CPI\",\\\n",
    "                           \"Unemployment\",\\\n",
    "                           \"IsHoliday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/sales.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has to be transformed into the correct datatypes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = sales\\\n",
    "    .withColumn(\"Store\", sales.Store.cast(IntegerType()))\\\n",
    "    .withColumn(\"Dept\", sales.Dept.cast(IntegerType()))\\\n",
    "    .withColumn(\"Date\", to_date(sales.Date, \"dd/MM/yyyy\"))\\\n",
    "    .withColumn(\"Weekly_Sales\", sales.Weekly_Sales.cast(DoubleType()))\\\n",
    "    .withColumn(\"IsHoliday\", sales.IsHoliday.cast(BooleanType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sales.join(features, [\"Store\", \"Date\", \"IsHoliday\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.orderBy('Date').tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The data ranges from 2010-02-05 (week 5) until 2012-10-26 (week 43 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Features/Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import weekofyear, year\n",
    "\n",
    "\n",
    "# extract year and week from date for further analysis\n",
    "df = df.withColumn('Week', weekofyear(df.Date))\n",
    "df = df.withColumn('Year', year(df.Date))\n",
    "df = df.withColumn('IsHoliday', df.IsHoliday.cast(IntegerType()))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null entries by comparing count with null rows removed\n",
    "df.na.drop().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "\n",
    "register_matplotlib_converters()\n",
    "\n",
    "# group by date and sum weekly sales over all stores and departements\n",
    "df_grouped =df_pd.groupby(by=['Date'], as_index=False)['Weekly_Sales'].sum()\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weekly_Sales')\n",
    "plt.plot(df_grouped['Date'], df_grouped['Weekly_Sales'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are obvious peaks/trends in the weekly sales during the holidays at the end of 2010 and 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_all =df_pd.groupby(by=['Date'], as_index=False)['Weekly_Sales',\\\n",
    "                                                         'Temperature',\\\n",
    "                                                         'Fuel_Price',\\\n",
    "                                                         'CPI',\\\n",
    "                                                         'Unemployment'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize correlation of sales and unemployment\n",
    "df_group_all['Unemployment'] = df_group_all['Unemployment']*2000\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df_group_all['Date'], df_group_all['Weekly_Sales'])\n",
    "plt.plot(df_group_all['Date'], df_group_all['Unemployment'])\n",
    "plt.xlabel('Date')\n",
    "plt.legend(('Weekly_Sales', 'Unemployment'))\n",
    "plt.savefig('out/unemployment.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that there is no correlation between weekly sales and unemployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize correlation of sales and temperature\n",
    "df_group_all['Temperature'] = df_group_all['Temperature']*250\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df_group_all['Date'], df_group_all['Weekly_Sales'])\n",
    "plt.plot(df_group_all['Date'], df_group_all['Temperature'])\n",
    "plt.xlabel('Date')\n",
    "plt.legend(('Weekly_Sales', 'Temperature'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that there is no direct correlation between temperature and sales. Nevertheless, theres is some kind of repetitive pattern with the same frequency. This supports the assumption that there is some kind of seasonal trend, as temperatures change according to the season."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize correlation of sales and fuel price\n",
    "df_group_all['Fuel_Price'] = df_group_all['Fuel_Price']*2000\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df_group_all['Date'], df_group_all['Weekly_Sales'])\n",
    "plt.plot(df_group_all['Date'], df_group_all['Fuel_Price'])\n",
    "plt.xlabel('Date')\n",
    "plt.legend(('Weekly_Sales', 'Fuel_Price'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that there is no correlation between weekly sales and fuel price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize correlation of sales and CPI (consumer price index)\n",
    "df_group_all['CPI'] = df_group_all['CPI']*100\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df_group_all['Date'], df_group_all['Weekly_Sales'])\n",
    "plt.plot(df_group_all['Date'], df_group_all['CPI'])\n",
    "plt.xlabel('Date')\n",
    "plt.legend(('Weekly_Sales', 'CPI'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows that there is also no correlation between weekly sales and CPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare weekly sales during the year\n",
    "df_2010 = df_pd[df_pd['Year']==2010]\n",
    "df_2010_grouped =df_2010.groupby(by=['Week'], as_index=False)['Weekly_Sales'].sum()\n",
    "\n",
    "df_2011 = df_pd[df_pd['Year']==2011]\n",
    "df_2011_grouped =df_2011.groupby(by=['Week'], as_index=False)['Weekly_Sales'].sum()\n",
    "\n",
    "df_2012 = df_pd[df_pd['Year']==2012]\n",
    "df_2012_grouped =df_2012.groupby(by=['Week'], as_index=False)['Weekly_Sales'].sum()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(df_2010_grouped['Week'], df_2010_grouped['Weekly_Sales'])\n",
    "plt.plot(df_2011_grouped['Week'], df_2011_grouped['Weekly_Sales'])\n",
    "plt.plot(df_2012_grouped['Week'], df_2012_grouped['Weekly_Sales'])\n",
    "\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Weekly_Sales')\n",
    "plt.legend(('2010', '2011', '2012'))\n",
    "plt.savefig('out/years.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "It does not make any sense to consider temperature, unemployment, CPI or fuel price as features. As illustrated in the l ast graph, there only pattern visible on first glance is a seasonal trend. Therefore, I decided to proceed with a time-series model. I chose to use Holt-Winters exponential smoothing due to the seasonal trend in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group_sales = df_pd.groupby('Date', as_index=False)['Weekly_Sales'].sum()\n",
    "df_group_sales = df_group_sales.set_index(\"Date\")\n",
    "\n",
    "df_group_sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in training and test data\n",
    "split_point = 100\n",
    "\n",
    "# take first x entries as train data\n",
    "train = df_group_sales.iloc[:split_point]\n",
    "# test with remaining entries\n",
    "test = df_group_sales.iloc[split_point:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "\n",
    "# fit exponential smoothing model\n",
    "fit_model = ExponentialSmoothing(train,\n",
    "                                  seasonal='mul',\n",
    "                                  seasonal_periods=52,\n",
    "                                  freq='W-FRI'\n",
    "                                ).fit()\n",
    "prediction = fit_model.forecast(len(df_group_sales)-split_point)\n",
    "prediction\n",
    "\n",
    "\n",
    "# plot predictions\n",
    "test.plot(figsize=(12,6))\n",
    "prediction.plot()\n",
    "plt.legend(('Weekly_Sales','Prediction'))\n",
    "plt.ylabel('Sum of Sales')\n",
    "plt.savefig('out/prediction.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "print(\"Mean Absolute Percentage Error = {a}%\".format(a=mean_absolute_percentage_error(test['Weekly_Sales'],prediction)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponential Smoothing seems to do a pretty good job. The current ratio is 100 entries for training and 43 entries for testing. \n",
    "The mean absolute percentage error will go down if the training set is extended, i.e. taking a ratio of 130 entries for training and the remaining 13 entries for testing will result in an error of 1.48% instead of 2.66% "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
